{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= pd.read_csv('train.csv')\n",
    "test_df= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Relations</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>position</th>\n",
       "      <th>sen_without_entity</th>\n",
       "      <th>sen_pos</th>\n",
       "      <th>pos_e1</th>\n",
       "      <th>pos_e2</th>\n",
       "      <th>enr_e1</th>\n",
       "      <th>enr_e2</th>\n",
       "      <th>root_e1</th>\n",
       "      <th>root_e2</th>\n",
       "      <th>shortest_dep_path</th>\n",
       "      <th>words_in_between</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;e1&gt; Thom Yorke &lt;/e1&gt; of &lt;e2&gt; Radiohead &lt;/e2&gt;...</td>\n",
       "      <td>per:employee_of(e1,e2)</td>\n",
       "      <td>Thom Yorke</td>\n",
       "      <td>Radiohead</td>\n",
       "      <td>[0, 3, 5, 7]</td>\n",
       "      <td>Thom Yorke of Radiohead have include the + for...</td>\n",
       "      <td>['PROPN', 'PROPN', 'ADP', 'PROPN', 'AUX', 'VER...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yorke</td>\n",
       "      <td>Radiohead</td>\n",
       "      <td>of</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;e1&gt; Leland High School &lt;/e1&gt; is a public hig...</td>\n",
       "      <td>org:city_of_headquarters(e1,e2)</td>\n",
       "      <td>Leland High School</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>[0, 4, 16, 19]</td>\n",
       "      <td>Leland High School be a public high school loc...</td>\n",
       "      <td>['PROPN', 'PROPN', 'PROPN', 'AUX', 'DET', 'ADJ...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "      <td>GPE</td>\n",
       "      <td>School</td>\n",
       "      <td>Jose</td>\n",
       "      <td>district</td>\n",
       "      <td>public high school locate Almaden Valley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 2008 Ohio Bobcats football team represent...</td>\n",
       "      <td>org:members(e2,e1)</td>\n",
       "      <td>Ohio University</td>\n",
       "      <td>NCAA</td>\n",
       "      <td>[7, 10, 14, 16]</td>\n",
       "      <td>the 2008 Ohio Bobcats football team represent ...</td>\n",
       "      <td>['DET', 'NUM', 'PROPN', 'PROPN', 'NOUN', 'NOUN...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>University</td>\n",
       "      <td>NCAA</td>\n",
       "      <td>represented team the division</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e1&gt; Holy Cross High School &lt;/e1&gt; is a Cathol...</td>\n",
       "      <td>org:founded_by(e1,e2)</td>\n",
       "      <td>Holy Cross High School</td>\n",
       "      <td>Congregation of Holy Cross</td>\n",
       "      <td>[0, 5, 19, 24]</td>\n",
       "      <td>Holy Cross High School be a catholic secondary...</td>\n",
       "      <td>['PROPN', 'PROPN', 'PROPN', 'PROPN', 'AUX', 'D...</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>School</td>\n",
       "      <td>Congregation</td>\n",
       "      <td>founded by</td>\n",
       "      <td>catholic secondary school found Waterbury Conn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hastings was unable to confirm news reports t...</td>\n",
       "      <td>per:employee_of(e2,e1)</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Bill Gwatney</td>\n",
       "      <td>[13, 15, 18, 21]</td>\n",
       "      <td>Hastings be unable to confirm news report that...</td>\n",
       "      <td>['PROPN', 'AUX', 'ADJ', 'PART', 'VERB', 'NOUN'...</td>\n",
       "      <td>PART</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Gwatney</td>\n",
       "      <td>party chairman</td>\n",
       "      <td>Democratic Party</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  \\\n",
       "0   <e1> Thom Yorke </e1> of <e2> Radiohead </e2>...   \n",
       "1   <e1> Leland High School </e1> is a public hig...   \n",
       "2   The 2008 Ohio Bobcats football team represent...   \n",
       "3   <e1> Holy Cross High School </e1> is a Cathol...   \n",
       "4   Hastings was unable to confirm news reports t...   \n",
       "\n",
       "                         Relations                      e1  \\\n",
       "0           per:employee_of(e1,e2)              Thom Yorke   \n",
       "1  org:city_of_headquarters(e1,e2)      Leland High School   \n",
       "2               org:members(e2,e1)         Ohio University   \n",
       "3            org:founded_by(e1,e2)  Holy Cross High School   \n",
       "4           per:employee_of(e2,e1)              Democratic   \n",
       "\n",
       "                           e2          position  \\\n",
       "0                   Radiohead      [0, 3, 5, 7]   \n",
       "1                    San Jose    [0, 4, 16, 19]   \n",
       "2                        NCAA   [7, 10, 14, 16]   \n",
       "3  Congregation of Holy Cross    [0, 5, 19, 24]   \n",
       "4                Bill Gwatney  [13, 15, 18, 21]   \n",
       "\n",
       "                                  sen_without_entity  \\\n",
       "0  Thom Yorke of Radiohead have include the + for...   \n",
       "1  Leland High School be a public high school loc...   \n",
       "2  the 2008 Ohio Bobcats football team represent ...   \n",
       "3  Holy Cross High School be a catholic secondary...   \n",
       "4  Hastings be unable to confirm news report that...   \n",
       "\n",
       "                                             sen_pos pos_e1 pos_e2 enr_e1  \\\n",
       "0  ['PROPN', 'PROPN', 'ADP', 'PROPN', 'AUX', 'VER...  PROPN  PROPN    NaN   \n",
       "1  ['PROPN', 'PROPN', 'PROPN', 'AUX', 'DET', 'ADJ...  PROPN  PROPN    ORG   \n",
       "2  ['DET', 'NUM', 'PROPN', 'PROPN', 'NOUN', 'NOUN...  PROPN  PROPN    ORG   \n",
       "3  ['PROPN', 'PROPN', 'PROPN', 'PROPN', 'AUX', 'D...  PROPN  PROPN    ORG   \n",
       "4  ['PROPN', 'AUX', 'ADJ', 'PART', 'VERB', 'NOUN'...   PART   NOUN    NaN   \n",
       "\n",
       "   enr_e2     root_e1       root_e2              shortest_dep_path  \\\n",
       "0     NaN       Yorke     Radiohead                             of   \n",
       "1     GPE      School          Jose                       district   \n",
       "2     ORG  University          NCAA  represented team the division   \n",
       "3     NaN      School  Congregation                     founded by   \n",
       "4  PERSON  Democratic       Gwatney                 party chairman   \n",
       "\n",
       "                                    words_in_between  \n",
       "0                                                NaN  \n",
       "1           public high school locate Almaden Valley  \n",
       "2                                               2008  \n",
       "3  catholic secondary school found Waterbury Conn...  \n",
       "4                                   Democratic Party  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode pos_e1,pos_e2, enr_e1, enr_e2 using Binary Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features= train_df[['pos_e1', 'pos_e2', 'enr_e1', 'enr_e2']]\n",
    "test_features= test_df[['pos_e1', 'pos_e2', 'enr_e1', 'enr_e2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "enc= ce.BinaryEncoder()\n",
    "enc= enc.fit(train_features)\n",
    "train_enc= enc.transform(train_features)\n",
    "test_enc= enc.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"ce.obj\",\"wb\")\n",
    "pickle.dump(enc, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_e1_0</th>\n",
       "      <th>pos_e1_1</th>\n",
       "      <th>pos_e1_2</th>\n",
       "      <th>pos_e1_3</th>\n",
       "      <th>pos_e1_4</th>\n",
       "      <th>pos_e1_5</th>\n",
       "      <th>pos_e2_0</th>\n",
       "      <th>pos_e2_1</th>\n",
       "      <th>pos_e2_2</th>\n",
       "      <th>pos_e2_3</th>\n",
       "      <th>...</th>\n",
       "      <th>enr_e1_0</th>\n",
       "      <th>enr_e1_1</th>\n",
       "      <th>enr_e1_2</th>\n",
       "      <th>enr_e1_3</th>\n",
       "      <th>enr_e1_4</th>\n",
       "      <th>enr_e2_0</th>\n",
       "      <th>enr_e2_1</th>\n",
       "      <th>enr_e2_2</th>\n",
       "      <th>enr_e2_3</th>\n",
       "      <th>enr_e2_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17255.0</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.0</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17255.0</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.0</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "      <td>17255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0.124775</td>\n",
       "      <td>0.090351</td>\n",
       "      <td>0.910693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.060794</td>\n",
       "      <td>0.084092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.199478</td>\n",
       "      <td>0.475514</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.288032</td>\n",
       "      <td>0.438482</td>\n",
       "      <td>0.486236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047489</td>\n",
       "      <td>0.232894</td>\n",
       "      <td>0.330474</td>\n",
       "      <td>0.286692</td>\n",
       "      <td>0.285195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036486</td>\n",
       "      <td>0.238959</td>\n",
       "      <td>0.277533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065787</td>\n",
       "      <td>0.399620</td>\n",
       "      <td>0.499415</td>\n",
       "      <td>0.486401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069191</td>\n",
       "      <td>0.452859</td>\n",
       "      <td>0.496215</td>\n",
       "      <td>0.499825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pos_e1_0      pos_e1_1      pos_e1_2      pos_e1_3      pos_e1_4  \\\n",
       "count   17255.0  17255.000000  17255.000000  17255.000000  17255.000000   \n",
       "mean        0.0      0.002260      0.057549      0.124775      0.090351   \n",
       "std         0.0      0.047489      0.232894      0.330474      0.286692   \n",
       "min         0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "max         0.0      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pos_e1_5  pos_e2_0      pos_e2_1      pos_e2_2      pos_e2_3  ...  \\\n",
       "count  17255.000000   17255.0  17255.000000  17255.000000  17255.000000  ...   \n",
       "mean       0.910693       0.0      0.001333      0.060794      0.084092  ...   \n",
       "std        0.285195       0.0      0.036486      0.238959      0.277533  ...   \n",
       "min        0.000000       0.0      0.000000      0.000000      0.000000  ...   \n",
       "25%        1.000000       0.0      0.000000      0.000000      0.000000  ...   \n",
       "50%        1.000000       0.0      0.000000      0.000000      0.000000  ...   \n",
       "75%        1.000000       0.0      0.000000      0.000000      0.000000  ...   \n",
       "max        1.000000       0.0      1.000000      1.000000      1.000000  ...   \n",
       "\n",
       "       enr_e1_0      enr_e1_1      enr_e1_2      enr_e1_3      enr_e1_4  \\\n",
       "count   17255.0  17255.000000  17255.000000  17255.000000  17255.000000   \n",
       "mean        0.0      0.004347      0.199478      0.475514      0.615879   \n",
       "std         0.0      0.065787      0.399620      0.499415      0.486401   \n",
       "min         0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.0      0.000000      0.000000      0.000000      1.000000   \n",
       "75%         0.0      0.000000      0.000000      1.000000      1.000000   \n",
       "max         0.0      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       enr_e2_0      enr_e2_1      enr_e2_2      enr_e2_3      enr_e2_4  \n",
       "count   17255.0  17255.000000  17255.000000  17255.000000  17255.000000  \n",
       "mean        0.0      0.004810      0.288032      0.438482      0.486236  \n",
       "std         0.0      0.069191      0.452859      0.496215      0.499825  \n",
       "min         0.0      0.000000      0.000000      0.000000      0.000000  \n",
       "25%         0.0      0.000000      0.000000      0.000000      0.000000  \n",
       "50%         0.0      0.000000      0.000000      0.000000      0.000000  \n",
       "75%         0.0      0.000000      1.000000      1.000000      1.000000  \n",
       "max         0.0      1.000000      1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_e1_0</th>\n",
       "      <th>pos_e1_1</th>\n",
       "      <th>pos_e1_2</th>\n",
       "      <th>pos_e1_3</th>\n",
       "      <th>pos_e1_4</th>\n",
       "      <th>pos_e1_5</th>\n",
       "      <th>pos_e2_0</th>\n",
       "      <th>pos_e2_1</th>\n",
       "      <th>pos_e2_2</th>\n",
       "      <th>pos_e2_3</th>\n",
       "      <th>...</th>\n",
       "      <th>enr_e1_0</th>\n",
       "      <th>enr_e1_1</th>\n",
       "      <th>enr_e1_2</th>\n",
       "      <th>enr_e1_3</th>\n",
       "      <th>enr_e1_4</th>\n",
       "      <th>enr_e2_0</th>\n",
       "      <th>enr_e2_1</th>\n",
       "      <th>enr_e2_2</th>\n",
       "      <th>enr_e2_3</th>\n",
       "      <th>enr_e2_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3331.0</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.0</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3331.0</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.0</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "      <td>3331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.066346</td>\n",
       "      <td>0.126088</td>\n",
       "      <td>0.095767</td>\n",
       "      <td>0.906635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.075353</td>\n",
       "      <td>0.094266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004803</td>\n",
       "      <td>0.195137</td>\n",
       "      <td>0.472231</td>\n",
       "      <td>0.618733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.266587</td>\n",
       "      <td>0.448514</td>\n",
       "      <td>0.502552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.248924</td>\n",
       "      <td>0.331999</td>\n",
       "      <td>0.294316</td>\n",
       "      <td>0.290987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038720</td>\n",
       "      <td>0.263999</td>\n",
       "      <td>0.292242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069150</td>\n",
       "      <td>0.396365</td>\n",
       "      <td>0.499303</td>\n",
       "      <td>0.485771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064703</td>\n",
       "      <td>0.442241</td>\n",
       "      <td>0.497417</td>\n",
       "      <td>0.500069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pos_e1_0     pos_e1_1     pos_e1_2     pos_e1_3     pos_e1_4  \\\n",
       "count    3331.0  3331.000000  3331.000000  3331.000000  3331.000000   \n",
       "mean        0.0     0.002101     0.066346     0.126088     0.095767   \n",
       "std         0.0     0.045800     0.248924     0.331999     0.294316   \n",
       "min         0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "25%         0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "50%         0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "75%         0.0     0.000000     0.000000     0.000000     0.000000   \n",
       "max         0.0     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          pos_e1_5  pos_e2_0     pos_e2_1     pos_e2_2     pos_e2_3  ...  \\\n",
       "count  3331.000000    3331.0  3331.000000  3331.000000  3331.000000  ...   \n",
       "mean      0.906635       0.0     0.001501     0.075353     0.094266  ...   \n",
       "std       0.290987       0.0     0.038720     0.263999     0.292242  ...   \n",
       "min       0.000000       0.0     0.000000     0.000000     0.000000  ...   \n",
       "25%       1.000000       0.0     0.000000     0.000000     0.000000  ...   \n",
       "50%       1.000000       0.0     0.000000     0.000000     0.000000  ...   \n",
       "75%       1.000000       0.0     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000       0.0     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "       enr_e1_0     enr_e1_1     enr_e1_2     enr_e1_3     enr_e1_4  enr_e2_0  \\\n",
       "count    3331.0  3331.000000  3331.000000  3331.000000  3331.000000    3331.0   \n",
       "mean        0.0     0.004803     0.195137     0.472231     0.618733       0.0   \n",
       "std         0.0     0.069150     0.396365     0.499303     0.485771       0.0   \n",
       "min         0.0     0.000000     0.000000     0.000000     0.000000       0.0   \n",
       "25%         0.0     0.000000     0.000000     0.000000     0.000000       0.0   \n",
       "50%         0.0     0.000000     0.000000     0.000000     1.000000       0.0   \n",
       "75%         0.0     0.000000     0.000000     1.000000     1.000000       0.0   \n",
       "max         0.0     1.000000     1.000000     1.000000     1.000000       0.0   \n",
       "\n",
       "          enr_e2_1     enr_e2_2     enr_e2_3     enr_e2_4  \n",
       "count  3331.000000  3331.000000  3331.000000  3331.000000  \n",
       "mean      0.004203     0.266587     0.448514     0.502552  \n",
       "std       0.064703     0.442241     0.497417     0.500069  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     1.000000  \n",
       "75%       0.000000     1.000000     1.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc.drop(['pos_e1_0', 'pos_e2_0', 'enr_e1_0', 'enr_e2_0'], axis=1, inplace=True)\n",
    "test_enc.drop(['pos_e1_0', 'pos_e2_0', 'enr_e1_0', 'enr_e2_0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17255, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc.to_csv('train_encodings_pos_enr.csv', index=False)\n",
    "test_enc.to_csv('test_encodings_pos_enr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode e1 and e2 using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sen):\n",
    "    '''\n",
    "    each word in 100 dimension, sum up these vectors for each word in sentence, to get one final 100 size vector. \n",
    "    '''\n",
    "    encoding=[0]*100\n",
    "    words= str(sen).split()\n",
    "    for word in words:\n",
    "        if word!='nan':\n",
    "            if word in model: \n",
    "                encoding +=model[word]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_entity(df):\n",
    "    enc_e1= df.e1.apply(encode).apply(pd.Series)\n",
    "    enc_e2= df.e2.apply(encode).apply(pd.Series)\n",
    "    \n",
    "    return pd.concat([enc_e1, enc_e2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_encodings_e1_e2= encode_entity(train_df)\n",
    "test_encodings_e1_e2= encode_entity(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17255, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings_e1_e2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3331, 200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings_e1_e2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings_e1_e2.to_csv('train_encodings_e1_e2.csv', index=False)\n",
    "test_encodings_e1_e2.to_csv('test_encodings_e1_e2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode shortest dependency path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train_encodings_SDP= train_df.shortest_dep_path.apply(encode).apply(pd.Series)\n",
    "test_encodings_SDP= test_df.shortest_dep_path.apply(encode).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17255, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings_SDP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings_SDP.to_csv('train_encodings_SDP.csv', index=False)\n",
    "test_encodings_SDP.to_csv('test_encodings_SDP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode e1 e2 with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length(sen):\n",
    "    words= sen.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(entity, max_len_entity):\n",
    "    words= entity.split()\n",
    "    enc=[]\n",
    "    padding=[0]*100\n",
    "    \n",
    "    for word in words:\n",
    "        enc.extend(model[word].tolist())\n",
    "    \n",
    "    pads= max_len_entity-len(words)\n",
    "    for pad in range(pads):\n",
    "        enc.extend(padding)\n",
    "        \n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find length of entities\n",
    "len_e1= train_df.e1.apply(find_length)\n",
    "len_e2= train_df.e2.apply(find_length)\n",
    "max_len_e1= max(len_e1)\n",
    "max_len_e2= max(len_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_entity(df):\n",
    "    # embed each word in entity and zero pad if less than the max length for both train and test df\n",
    "    enc_e1= df.apply(lambda row: get_encoding(row.e1, max_len_e1), axis=1).apply(pd.Series)\n",
    "    enc_e2= df.apply(lambda row: get_encoding(row.e2, max_len_e2), axis=1).apply(pd.Series)\n",
    "    enc_e1_e2= pd.concat([enc_e1, enc_e2], axis=1)\n",
    "    \n",
    "    return enc_e1_e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train_enc_e1_e2= encode_entity(train_df)\n",
    "test_enc_e1_e2= encode_entity(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17255, 2000) (3331, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(train_enc_e1_e2.shape, test_enc_e1_e2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_e1_e2.to_csv('train_encodings_e1_e2_padding.csv', index=False)\n",
    "test_enc_e1_e2.to_csv('test_encodings_e1_e2_padding.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc= LabelEncoder()\n",
    "enc_label= enc.fit(train_df.Relations)\n",
    "train_enc_label= enc.transform(train_df.Relations)\n",
    "test_enc_label= enc.transform(test_df.Relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train_enc_label).to_csv('train_label_enc.csv', index=False)\n",
    "pd.Series(test_enc_label).to_csv('test_label_enc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode words in between with word embeddings 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_enc_words_in_between= train_df.words_in_between.apply(encode).apply(pd.Series)\n",
    "test_enc_words_in_between= test_df.words_in_between.apply(encode).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17255, 100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc_words_in_between.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3331, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_words_in_between.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_words_in_between.to_csv('train_enc_words_in_between.csv', index=False)\n",
    "test_enc_words_in_between.to_csv('test_enc_words_in_between.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_root(df):\n",
    "    df_enc_root_e1= df.root_e1.apply(encode).apply(pd.Series)\n",
    "    df_enc_root_e2= df.root_e2.apply(encode).apply(pd.Series)\n",
    "    enc_root= pd.concat([df_enc_root_e1, df_enc_root_e2], axis=1)\n",
    "    \n",
    "    return enc_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train_enc_root= encode_root(train_df)\n",
    "test_enc_root= encode_root(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17255, 200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc_root.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3331, 200)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc_root.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_root.to_csv('train_enc_root.csv', index=False)\n",
    "test_enc_root.to_csv('test_enc_root.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode all into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_enr= pd.read_csv('train_encodings_pos_enr.csv')\n",
    "test_pos_enr= pd.read_csv('test_encodings_pos_enr.csv')\n",
    "# e1 and e2\n",
    "train_e1_e2= pd.read_csv('train_encodings_e1_e2.csv')\n",
    "test_e1_e2= pd.read_csv('test_encodings_e1_e2.csv')\n",
    "# SDP\n",
    "train_SDP= pd.read_csv('train_encodings_SDP.csv')\n",
    "test_SDP= pd.read_csv('test_encodings_SDP.csv')\n",
    "# words in between \n",
    "train_words_in_between= pd.read_csv('train_enc_words_in_between.csv')\n",
    "test_words_in_between= pd.read_csv('test_enc_words_in_between.csv')\n",
    "# root words\n",
    "train_root= pd.read_csv('train_enc_root.csv')\n",
    "test_root= pd.read_csv('test_enc_root.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_enr_e1e2_root_between= pd.concat([train_pos_enr, train_e1_e2, train_root, train_words_in_between, train_SDP], axis=1)\n",
    "test_pos_enr_e1e2_root_between= pd.concat([test_pos_enr, test_e1_e2, test_root, test_words_in_between, test_SDP], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_enr_e1e2_root_between.to_csv('train_pos_enr_e1e2_root_between.csv', index=False)\n",
    "test_pos_enr_e1e2_root_between.to_csv('test_pos_enr_e1e2_root_between.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
