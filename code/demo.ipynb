{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "stopwords= stopwords.words('english')\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_index(sen):\n",
    "    sen_list= sen.split()\n",
    "\n",
    "    for i, word in enumerate(sen_list):\n",
    "        if word=='<e1>':\n",
    "            start1=i\n",
    "\n",
    "        elif word=='</e1>':\n",
    "            end1=i\n",
    "\n",
    "        if word=='<e2>':\n",
    "            start2=i\n",
    "\n",
    "        elif word=='</e2>':\n",
    "            end2=i\n",
    "            \n",
    "    # get e1 and e2\n",
    "    e1= \" \".join(sen_list[start1+1 : end1])\n",
    "    e2= \" \".join(sen_list[start2+1 : end2])\n",
    "    \n",
    "    return e1, e2, [start1, end1, start2, end2]\n",
    "\n",
    "def get_sen_without_entity(sen):\n",
    "    sen_list= sen.split()\n",
    "    sen_without_entity= \" \".join([token for token in sen_list if token not in {'<e1>','</e1>', '<e2>', '</e2>'}]) \n",
    "    return sen_without_entity\n",
    "\n",
    "def get_ner(entity):\n",
    "    for ent in nlp(entity).ents:\n",
    "        return str(ent.label_)\n",
    "\n",
    "def shortest_dep_path(tokens, root_e1, root_e2):\n",
    "    \n",
    "    #print dependency tree \n",
    "    #displacy.render(doc,jupyter=True)\n",
    "\n",
    "    # Load spacy's dependency tree into a networkx graph\n",
    "    edges = []\n",
    "    for token in tokens:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token.lower_),\n",
    "                          '{0}'.format(child.lower_)))\n",
    "            \n",
    "    graph = nx.Graph(edges)\n",
    "    entity1 = root_e1.lower()\n",
    "    entity2 = root_e2.lower()\n",
    "    \n",
    "    try:\n",
    "        out = str(\" \".join(nx.shortest_path(graph, source=entity1, target=entity2)[1:-1]))\n",
    "        \n",
    "    except (nx.NetworkXNoPath,  nx.NodeNotFound) as e:\n",
    "        out= None\n",
    "    \n",
    "    return out\n",
    "\n",
    "def extracting_sysNet(sen_without_entity):\n",
    "    words= sen_without_entity.split()\n",
    "    \n",
    "    word_features_hypernymy={}\n",
    "    word_features_hyponymy={}\n",
    "    word_features_meronym={}\n",
    "    word_features_holonymy={}\n",
    "    \n",
    "    for word in words:\n",
    "        word_features_hypernymy[word]=[]\n",
    "        word_features_hyponymy[word]=[]\n",
    "        word_features_meronym[word]=[]\n",
    "        word_features_holonymy[word]=[]\n",
    "\n",
    "    for word in words:  \n",
    "        for i,j in enumerate(wn.synsets(word)):\n",
    "\n",
    "            #Hypernymy\n",
    "            for hyper in j.hypernyms():\n",
    "                Hypernyms= hyper.lemma_names()\n",
    "                for entry in Hypernyms:\n",
    "                    word_features_hypernymy[word].append(entry)\n",
    "\n",
    "            #Hyponymy\n",
    "            for hypo in j.hyponyms():\n",
    "                Hyponyms=hypo.lemma_names()\n",
    "                for h in Hyponyms:\n",
    "                    word_features_hyponymy[word].append(h)\n",
    "\n",
    "            #Meronyms\n",
    "            for mem in j.part_meronyms():\n",
    "                Meronyms=mem.lemma_names()\n",
    "                for m in Meronyms:\n",
    "                    word_features_meronym[word].append(m)\n",
    "\n",
    "            #Holonyms\n",
    "            for holo in j.part_holonyms():\n",
    "                Holonyms=holo.lemma_names()\n",
    "                for ho in Holonyms:\n",
    "                    word_features_holonymy[word].append(ho)\n",
    "\n",
    "    return word_features_hypernymy, word_features_hyponymy, word_features_meronym, word_features_holonymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sen):\n",
    "    doc= nlp(sen)\n",
    "    return [token for token in doc]\n",
    "        \n",
    "def lemmatize(tokens):\n",
    "    return [token.lemma_ for token in tokens]\n",
    "        \n",
    "def get_pos_sen(tokens):\n",
    "    return [token.pos_ for token in tokens]\n",
    "\n",
    "# get root\n",
    "def get_root(entity):\n",
    "    # create a span object that has property .root\n",
    "    doc = nlp(entity)\n",
    "    sen= list(doc.sents)[0]\n",
    "    return str(sen.root)      \n",
    "\n",
    "def get_words_in_between(sen_without_entity, position):\n",
    "    '''\n",
    "    get the words in between entities which are not stop words\n",
    "    '''\n",
    "    words= sen_without_entity.split()\n",
    "    words_in_between= words[position[1]-1: position[2]-2]\n",
    "    return \" \".join([word for word in words_in_between if word not in stopwords])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen= \"The 2008 Ohio Bobcats football team represented <e1> Ohio University </e1> during the 2008 <e2> NCAA </e2> Division I FBS football season . \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen= \" On Wednesday <e1> Guyana </e1> 's President <e2> Bharrat Jagdeo </e2> publicly questioned why it's taking so long to get its first installment of funds under a $ 250 million forest conservation agreement with Norway . \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting entity and its position\n",
    "e1, e2, position= get_entity_index(sen)\n",
    "# removing entity tags\n",
    "sen_without_entity= get_sen_without_entity(sen)\n",
    "# tokenize, lemmatize, and POS tags from spacy\n",
    "tokens= tokenize(sen_without_entity)\n",
    "lemmas= lemmatize(tokens)\n",
    "pos_sen= get_pos_sen(tokens)\n",
    "# get pos_e1\n",
    "pos_e1= pos_sen[position[0]]\n",
    "pos_e2= pos_sen[position[2]-2]\n",
    "# root e1 and root e2\n",
    "root_e1= get_root(e1)\n",
    "root_e2= get_root(e2)\n",
    "# SDP\n",
    "SDP= shortest_dep_path(tokens, root_e1, root_e2)\n",
    "# SysNet\n",
    "word_features_hypernymy, word_features_hypernomy, word_features_meronym, word_features_holonymy= extracting_sysNet(sen_without_entity)\n",
    "# get NER of e1 and e2\n",
    "enr_e1, enr_e2= get_ner(e1), get_ner(e2)\n",
    "# get words in between \n",
    "words_in_between= get_words_in_between(sen_without_entity, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities:  Guyana Bharrat Jagdeo\n",
      "tokens:  [On, Wednesday, Guyana, 's, President, Bharrat, Jagdeo, publicly, questioned, why, it, 's, taking, so, long, to, get, its, first, installment, of, funds, under, a, $, 250, million, forest, conservation, agreement, with, Norway, .]\n",
      "lemmas:  ['on', 'Wednesday', 'Guyana', \"'s\", 'President', 'Bharrat', 'Jagdeo', 'publicly', 'question', 'why', '-PRON-', 'be', 'take', 'so', 'long', 'to', 'get', '-PRON-', 'first', 'installment', 'of', 'fund', 'under', 'a', '$', '250', 'million', 'forest', 'conservation', 'agreement', 'with', 'Norway', '.']\n",
      "pos:  ['ADP', 'PROPN', 'PROPN', 'PART', 'PROPN', 'PROPN', 'PROPN', 'ADV', 'VERB', 'ADV', 'PRON', 'AUX', 'VERB', 'ADV', 'ADV', 'PART', 'AUX', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'SYM', 'NUM', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'PROPN', 'PUNCT']\n",
      "SDP:  president bharrat\n",
      "word_features_hypernymy:  {'On': [], 'Wednesday': ['weekday'], 'Guyana': [], \"'s\": [], 'President': ['corporate_executive', 'business_executive', 'head_of_state', 'chief_of_state', 'head_of_state', 'chief_of_state', 'presiding_officer', 'academic_administrator', 'presidency', 'presidentship'], 'Bharrat': [], 'Jagdeo': [], 'publicly': [], 'questioned': ['challenge', 'ask', 'ask', 'converse', 'discourse', 'chew_over', 'think_over', 'meditate', 'ponder', 'excogitate', 'contemplate', 'muse', 'reflect', 'mull', 'mull_over', 'ruminate', 'speculate'], 'why': ['reason', 'ground'], \"it's\": [], 'taking': ['action', 'act', 'move', 'use', 'expend', 'change', 'interpret', 'construe', 'see', 'transport', 'carry', 'use', 'utilize', 'utilise', 'apply', 'employ', 'decide', \"make_up_one's_mind\", 'determine', 'get', 'acquire', 'work', 'do_work', 'think_about', 'experience', 'receive', 'have', 'get', 'record', 'enter', 'put_down', 'undergo', 'move', 'accept', 'take', 'have', 'receive', 'get', 'find', 'obtain', 'incur', 'necessitate', 'ask', 'postulate', 'need', 'require', 'take', 'involve', 'call_for', 'demand', 'head', 'position', 'become', 'go', 'get', 'have', 'feature', 'get', 'acquire', 'buy', 'purchase', 'buy', 'purchase', 'sleep_together', 'roll_in_the_hay', 'love', 'make_out', 'make_love', 'sleep_with', 'get_laid', 'have_sex', 'know', 'do_it', 'be_intimate', 'have_intercourse', 'have_it_away', 'have_it_off', 'screw', 'fuck', 'jazz', 'eff', 'hump', 'lie_with', 'bed', 'have_a_go_at_it', 'bang', 'get_it_on', 'bonk', 'affirm', 'verify', 'assert', 'avow', 'aver', 'swan', 'swear', 'be', 'be', 'traverse', 'track', 'cover', 'cross', 'pass_over', 'get_over', 'get_across', 'cut_through', 'cut_across', 'win', 'sicken', 'come_down'], 'so': ['solfa_syllable'], 'long': ['desire', 'want'], 'to': [], 'get': ['return', 'change_state', 'turn', 'make', 'get', 'change', 'transmit', 'transfer', 'transport', 'channel', 'channelize', 'channelise', 'undergo', 'get_even', 'get_back', 'score', 'hit', 'tally', 'rack_up', 'seize', 'prehend', 'clutch', 'change', 'sicken', 'come_down', 'communicate', 'intercommunicate', 'change', 'alter', 'modify', 'mean', 'intend', 'understand', 'attract', 'pull', 'pull_in', 'draw', 'draw_in', 'hit', 'get', 'acquire', 'buy', 'purchase', 'hear', 'hurt', 'ache', 'suffer', 'leave', 'go_forth', 'go_away', 'catch', 'annoy', 'rag', 'get_to', 'bother', 'get_at', 'irritate', 'rile', 'nark', 'nettle', 'gravel', 'vex', 'chafe', 'devil', 'touch', 'stir', 'reproduce', 'effect', 'effectuate', 'set_up', 'destroy', 'ruin', 'confuse', 'throw', 'fox', 'befuddle', 'fuddle', 'bedevil', 'confound', 'discombobulate', 'experience', 'receive', 'have', 'get', 'make', 'create'], 'its': ['engineering', 'engineering_science', 'applied_science', 'technology'], 'first': ['rank', 'ordinal_number', 'ordinal', 'no.', 'point', 'point_in_time', 'position', 'honours', 'honours_degree', 'gear', 'gear_mechanism'], 'installment': ['payment', 'broadcast', 'program', 'programme', 'text', 'textual_matter', 'beginning', 'start', 'commencement'], 'of': [], 'funds': ['assets', 'money', 'accumulation', 'nondepository_financial_institution', 'finance', 'roll_up', 'collect', 'accumulate', 'pile_up', 'amass', 'compile', 'hoard', 'supply', 'provide', 'render', 'furnish', 'invest', 'put', 'commit', 'place', 'roll_up', 'collect', 'accumulate', 'pile_up', 'amass', 'compile', 'hoard', 'support'], 'under': [], 'a': ['metric_linear_unit', 'fat-soluble_vitamin', 'nucleotide', 'base', 'purine', 'current_unit', 'letter', 'letter_of_the_alphabet', 'alphabetic_character', 'blood_group', 'blood_type'], '$': [], '250': [], 'million': ['large_integer', 'large_indefinite_quantity', 'large_indefinite_amount'], 'forest': ['vegetation', 'flora', 'botany', 'biome', 'land', 'dry_land', 'earth', 'ground', 'solid_ground', 'terra_firma', 'plant', 'set'], 'conservation': ['improvement', 'betterment', 'advance', 'preservation', 'saving', 'principle'], 'agreement': ['statement', 'compatibility', 'harmony', 'concord', 'concordance', 'planning', 'preparation', 'provision', 'grammatical_relation', 'speech_act'], 'with': [], 'Norway': [], '.': []}\n",
      "enr_e1:  PERSON\n",
      "enr_e2:  ORG\n",
      "pos_e1:  PROPN\n",
      "pos_e2:  PROPN\n",
      "words in between:  's President\n"
     ]
    }
   ],
   "source": [
    "print('entities: ', e1, e2)\n",
    "print('tokens: ', tokens)\n",
    "print('lemmas: ', lemmas)\n",
    "print('pos: ', pos_sen)\n",
    "print('SDP: ', SDP)\n",
    "print('word_features_hypernymy: ', word_features_hypernymy )\n",
    "print('enr_e1: ', enr_e1)\n",
    "print('enr_e2: ', enr_e2)\n",
    "print('pos_e1: ', pos_e1)\n",
    "print('pos_e2: ', pos_e2)\n",
    "print('words in between: ', words_in_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enr= pd.DataFrame([[pos_e1, pos_e2, enr_e1, enr_e2]], columns=['pos_e1', 'pos_e2', 'enr_e1', 'enr_e2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open('ce.obj', 'rb') \n",
    "ce= pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enr_enc= ce.transform(pos_enr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enr_enc.drop(['pos_e1_0', 'pos_e2_0', 'enr_e1_0', 'enr_e2_0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enr_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model= Word2Vec.load('word_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sen):\n",
    "    '''\n",
    "    each word in 100 dimension, sum up these vectors for each word in sentence, to get one final 100 size vector. \n",
    "    '''\n",
    "    encoding=np.zeros(100)\n",
    "    words= str(sen).split()\n",
    "    for word in words:\n",
    "        if word in word_model:\n",
    "            encoding += word_model[word]\n",
    "    return pd.DataFrame(encoding.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entity embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "e1_enc= encode(e1)\n",
    "e2_enc= encode(e2)\n",
    "e1_enc= e1_enc.add_prefix('e1_')\n",
    "e2_enc= e2_enc.add_prefix('e2_')\n",
    "entity_enc= pd.concat([e1_enc, e2_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e1_0</th>\n",
       "      <th>e1_1</th>\n",
       "      <th>e1_2</th>\n",
       "      <th>e1_3</th>\n",
       "      <th>e1_4</th>\n",
       "      <th>e1_5</th>\n",
       "      <th>e1_6</th>\n",
       "      <th>e1_7</th>\n",
       "      <th>e1_8</th>\n",
       "      <th>e1_9</th>\n",
       "      <th>...</th>\n",
       "      <th>e2_90</th>\n",
       "      <th>e2_91</th>\n",
       "      <th>e2_92</th>\n",
       "      <th>e2_93</th>\n",
       "      <th>e2_94</th>\n",
       "      <th>e2_95</th>\n",
       "      <th>e2_96</th>\n",
       "      <th>e2_97</th>\n",
       "      <th>e2_98</th>\n",
       "      <th>e2_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.11142</td>\n",
       "      <td>-0.266119</td>\n",
       "      <td>-0.156917</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.185912</td>\n",
       "      <td>0.275955</td>\n",
       "      <td>0.140905</td>\n",
       "      <td>0.228886</td>\n",
       "      <td>0.107982</td>\n",
       "      <td>-0.241018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035319</td>\n",
       "      <td>-0.540367</td>\n",
       "      <td>-0.33716</td>\n",
       "      <td>0.092133</td>\n",
       "      <td>0.240597</td>\n",
       "      <td>-0.294627</td>\n",
       "      <td>-0.022181</td>\n",
       "      <td>-0.121581</td>\n",
       "      <td>0.099459</td>\n",
       "      <td>0.011981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      e1_0      e1_1      e1_2      e1_3      e1_4      e1_5      e1_6  \\\n",
       "0  0.11142 -0.266119 -0.156917 -0.087371  0.185912  0.275955  0.140905   \n",
       "\n",
       "       e1_7      e1_8      e1_9  ...     e2_90     e2_91    e2_92     e2_93  \\\n",
       "0  0.228886  0.107982 -0.241018  ...  0.035319 -0.540367 -0.33716  0.092133   \n",
       "\n",
       "      e2_94     e2_95     e2_96     e2_97     e2_98     e2_99  \n",
       "0  0.240597 -0.294627 -0.022181 -0.121581  0.099459  0.011981  \n",
       "\n",
       "[1 rows x 200 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "sdp_enc= encode(SDP)\n",
    "sdp_enc= sdp_enc.add_prefix('sdp_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sdp_0</th>\n",
       "      <th>sdp_1</th>\n",
       "      <th>sdp_2</th>\n",
       "      <th>sdp_3</th>\n",
       "      <th>sdp_4</th>\n",
       "      <th>sdp_5</th>\n",
       "      <th>sdp_6</th>\n",
       "      <th>sdp_7</th>\n",
       "      <th>sdp_8</th>\n",
       "      <th>sdp_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sdp_90</th>\n",
       "      <th>sdp_91</th>\n",
       "      <th>sdp_92</th>\n",
       "      <th>sdp_93</th>\n",
       "      <th>sdp_94</th>\n",
       "      <th>sdp_95</th>\n",
       "      <th>sdp_96</th>\n",
       "      <th>sdp_97</th>\n",
       "      <th>sdp_98</th>\n",
       "      <th>sdp_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.945116</td>\n",
       "      <td>0.101658</td>\n",
       "      <td>-0.713092</td>\n",
       "      <td>-1.217193</td>\n",
       "      <td>-0.212802</td>\n",
       "      <td>0.113164</td>\n",
       "      <td>0.885969</td>\n",
       "      <td>-0.641224</td>\n",
       "      <td>0.833309</td>\n",
       "      <td>-0.7244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178008</td>\n",
       "      <td>-2.576855</td>\n",
       "      <td>-1.642165</td>\n",
       "      <td>1.138858</td>\n",
       "      <td>1.18302</td>\n",
       "      <td>-1.160019</td>\n",
       "      <td>-0.103197</td>\n",
       "      <td>-0.821332</td>\n",
       "      <td>1.523956</td>\n",
       "      <td>1.52422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sdp_0     sdp_1     sdp_2     sdp_3     sdp_4     sdp_5     sdp_6  \\\n",
       "0  0.945116  0.101658 -0.713092 -1.217193 -0.212802  0.113164  0.885969   \n",
       "\n",
       "      sdp_7     sdp_8   sdp_9  ...    sdp_90    sdp_91    sdp_92    sdp_93  \\\n",
       "0 -0.641224  0.833309 -0.7244  ... -0.178008 -2.576855 -1.642165  1.138858   \n",
       "\n",
       "    sdp_94    sdp_95    sdp_96    sdp_97    sdp_98   sdp_99  \n",
       "0  1.18302 -1.160019 -0.103197 -0.821332  1.523956  1.52422  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdp_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# words in between embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "words_bet_enc= encode(words_in_between)\n",
    "words_bet_enc= words_bet_enc.add_prefix('bet_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bet_0</th>\n",
       "      <th>bet_1</th>\n",
       "      <th>bet_2</th>\n",
       "      <th>bet_3</th>\n",
       "      <th>bet_4</th>\n",
       "      <th>bet_5</th>\n",
       "      <th>bet_6</th>\n",
       "      <th>bet_7</th>\n",
       "      <th>bet_8</th>\n",
       "      <th>bet_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bet_90</th>\n",
       "      <th>bet_91</th>\n",
       "      <th>bet_92</th>\n",
       "      <th>bet_93</th>\n",
       "      <th>bet_94</th>\n",
       "      <th>bet_95</th>\n",
       "      <th>bet_96</th>\n",
       "      <th>bet_97</th>\n",
       "      <th>bet_98</th>\n",
       "      <th>bet_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.508099</td>\n",
       "      <td>0.128895</td>\n",
       "      <td>-2.242232</td>\n",
       "      <td>-1.71071</td>\n",
       "      <td>-0.234733</td>\n",
       "      <td>1.241015</td>\n",
       "      <td>0.339153</td>\n",
       "      <td>-1.767375</td>\n",
       "      <td>2.063224</td>\n",
       "      <td>-2.154587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271633</td>\n",
       "      <td>-4.997834</td>\n",
       "      <td>-3.424843</td>\n",
       "      <td>1.567164</td>\n",
       "      <td>2.163761</td>\n",
       "      <td>-2.835466</td>\n",
       "      <td>-0.259345</td>\n",
       "      <td>-1.243096</td>\n",
       "      <td>2.618251</td>\n",
       "      <td>2.045603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bet_0     bet_1     bet_2    bet_3     bet_4     bet_5     bet_6  \\\n",
       "0  1.508099  0.128895 -2.242232 -1.71071 -0.234733  1.241015  0.339153   \n",
       "\n",
       "      bet_7     bet_8     bet_9  ...    bet_90    bet_91    bet_92    bet_93  \\\n",
       "0 -1.767375  2.063224 -2.154587  ...  0.271633 -4.997834 -3.424843  1.567164   \n",
       "\n",
       "     bet_94    bet_95    bet_96    bet_97    bet_98    bet_99  \n",
       "0  2.163761 -2.835466 -0.259345 -1.243096  2.618251  2.045603  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_bet_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\Gunjan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "root_e1_enc= encode(root_e1)\n",
    "root_e2_enc= encode(root_e2)\n",
    "root_e1_enc= root_e1_enc.add_prefix('root_e1_')\n",
    "root_e2_enc= root_e2_enc.add_prefix('root_e2_')\n",
    "root_enc= pd.concat([root_e1_enc, root_e2_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root_e1_0</th>\n",
       "      <th>root_e1_1</th>\n",
       "      <th>root_e1_2</th>\n",
       "      <th>root_e1_3</th>\n",
       "      <th>root_e1_4</th>\n",
       "      <th>root_e1_5</th>\n",
       "      <th>root_e1_6</th>\n",
       "      <th>root_e1_7</th>\n",
       "      <th>root_e1_8</th>\n",
       "      <th>root_e1_9</th>\n",
       "      <th>...</th>\n",
       "      <th>root_e2_90</th>\n",
       "      <th>root_e2_91</th>\n",
       "      <th>root_e2_92</th>\n",
       "      <th>root_e2_93</th>\n",
       "      <th>root_e2_94</th>\n",
       "      <th>root_e2_95</th>\n",
       "      <th>root_e2_96</th>\n",
       "      <th>root_e2_97</th>\n",
       "      <th>root_e2_98</th>\n",
       "      <th>root_e2_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.11142</td>\n",
       "      <td>-0.266119</td>\n",
       "      <td>-0.156917</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.185912</td>\n",
       "      <td>0.275955</td>\n",
       "      <td>0.140905</td>\n",
       "      <td>0.228886</td>\n",
       "      <td>0.107982</td>\n",
       "      <td>-0.241018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>-0.266751</td>\n",
       "      <td>-0.168431</td>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.119067</td>\n",
       "      <td>-0.143669</td>\n",
       "      <td>-0.004295</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>-0.002716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   root_e1_0  root_e1_1  root_e1_2  root_e1_3  root_e1_4  root_e1_5  \\\n",
       "0    0.11142  -0.266119  -0.156917  -0.087371   0.185912   0.275955   \n",
       "\n",
       "   root_e1_6  root_e1_7  root_e1_8  root_e1_9  ...  root_e2_90  root_e2_91  \\\n",
       "0   0.140905   0.228886   0.107982  -0.241018  ...    0.020098   -0.266751   \n",
       "\n",
       "   root_e2_92  root_e2_93  root_e2_94  root_e2_95  root_e2_96  root_e2_97  \\\n",
       "0   -0.168431    0.040973    0.119067   -0.143669   -0.004295   -0.056671   \n",
       "\n",
       "   root_e2_98  root_e2_99  \n",
       "0    0.036887   -0.002716  \n",
       "\n",
       "[1 rows x 200 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat all features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enr_e1e2_root_between= pd.concat([pos_enr_enc, entity_enc, root_enc, words_bet_enc, sdp_enc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 618)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enr_e1e2_root_between.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pickle.load(open('col.bin', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enr_e1e2_root_between.columns= cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "xgb_model = pickle.load(open('xgboost_model.bin', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction:  per:employee_of(e1,e2)\n",
      "Time taken to predict:  0.009009361267089844\n"
     ]
    }
   ],
   "source": [
    "start_time= time()\n",
    "print('Model prediction: ', xgb_model.predict(pos_enr_e1e2_root_between)[0])\n",
    "end_time= time()\n",
    "print('Time taken to predict: ', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
